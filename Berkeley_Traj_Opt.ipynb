{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from numpy.linalg import LinAlgError\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gen_param_dict(object):\n",
    "    '''\n",
    "    Init function not required. Currently there for debugging.\n",
    "        init: define the class constructor of with no arguments\n",
    "    '''\n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print('-'*25)\n",
    "            print('Call to the class gen_param_dict has started')\n",
    "            print('-'*25)\n",
    "        \n",
    "    def gen_param_ALG(self, inner_iterations=1, min_eta=1e-5, kl_step=0.2, min_step_mult=0.01,\n",
    "                     max_step_mult=10.0, min_mult=0.1, max_mult=5.0, initial_state_var=1e-6, \n",
    "                     init_traj_distr=None, traj_opt=None, max_ent_traj=0.0, dynamics=None, \n",
    "                     cost=None, sample_on_policy=False, fit_dynamics=True):\n",
    "        # Algorithm\n",
    "        ALG = {\n",
    "            # Number of iterations.\n",
    "            'inner_iterations': inner_iterations,  \n",
    "            # Minimum initial lagrange multiplier in DGD for trajectory optimization.\n",
    "            'min_eta': min_eta,        \n",
    "            'kl_step': kl_step,\n",
    "            'min_step_mult': min_step_mult,\n",
    "            'max_step_mult': max_step_mult,\n",
    "            'min_mult': min_mult,\n",
    "            'max_mult': max_mult,\n",
    "            # Trajectory settings.\n",
    "            'initial_state_var': initial_state_var,\n",
    "            # A list of initial LinearGaussianPolicy objects for each condition.\n",
    "            'init_traj_distr': init_traj_distr, \n",
    "            # Trajectory optimization.\n",
    "            'traj_opt': traj_opt,\n",
    "            # Weight of maximum entropy term in trajectory optimization.\n",
    "            'max_ent_traj': max_ent_traj,\n",
    "            # Dynamics hyperaparams.\n",
    "            'dynamics': dynamics,\n",
    "            # Costs.\n",
    "            'cost': cost,  # A list of Cost objects for each condition.\n",
    "            # Whether or not to sample with neural net policy (only for badmm/mdgps).\n",
    "            'sample_on_policy': sample_on_policy,\n",
    "            # Inidicates if the algorithm requires fitting of the dynamics.\n",
    "            'fit_dynamics': fit_dynamics,    \n",
    "        }\n",
    "        return ALG\n",
    "    \n",
    "    def gen_param_ALG_BADMM(self, inner_iterations=4.0, policy_dual_rate=0.1, policy_dual_rate_covar=0.0,\n",
    "                           fixed_lg_step=0.0, lg_step_schedule=10.0, ent_reg_schedule=0.0, init_pol_wt=0.01,\n",
    "                           policy_sample_mode='add', exp_step_increase=2.0, exp_step_decrease=0.5, \n",
    "                            exp_step_upper=0.5, exp_step_lower=1.0): \n",
    "        # Algorithm BADMM\n",
    "        ALG_BADMM = {\n",
    "            'inner_iterations': inner_iterations,\n",
    "            'policy_dual_rate': policy_dual_rate,\n",
    "            'policy_dual_rate_covar': policy_dual_rate_covar,\n",
    "            'fixed_lg_step': fixed_lg_step,\n",
    "            'lg_step_schedule': lg_step_schedule,\n",
    "            'ent_reg_schedule': ent_reg_schedule,\n",
    "            'init_pol_wt': init_pol_wt,\n",
    "            'policy_sample_mode': policy_sample_mode,\n",
    "            'exp_step_increase': exp_step_increase,\n",
    "            'exp_step_decrease': exp_step_decrease,\n",
    "            'exp_step_upper': exp_step_upper,\n",
    "            'exp_step_lower': exp_step_lower,\n",
    "        }\n",
    "        return ALG_BADMM\n",
    "    \n",
    "    def gen_param_TRAJ_OPT_LQR(self, del0=1e-4, eta_error_threshold=1e16, min_eta=1e-8, max_eta=1e16,\n",
    "                              cons_per_step=False, use_prev_distr=False, update_in_bwd_pass=True):\n",
    "        TRAJ_OPT_LQR = {\n",
    "            'del0': del0,\n",
    "            'eta_error_threshold': eta_error_threshold,\n",
    "            'min_eta': min_eta,\n",
    "            'max_eta': max_eta,\n",
    "            # Whether or not to enforce separate KL constraints at each time step.\n",
    "            'cons_per_step': cons_per_step,  \n",
    "            # Whether or not to measure expected KL under the previous traj distr.\n",
    "            'use_prev_distr': use_prev_distr,  \n",
    "            # Whether or not to update the TVLG controller during the bwd pass.\n",
    "            'update_in_bwd_pass': update_in_bwd_pass,  \n",
    "        }\n",
    "        return TRAJ_OPT_LQR\n",
    "    \n",
    "    def gen_param_INIT_LG_LQR(self, init_var=1.0, stiffness=1.0, stiffness_vel=0.5,\n",
    "                             final_weight=1.0):\n",
    "        # Initial Linear Gaussian Trajectory distribution, LQR-based initializer.\n",
    "        INIT_LG_LQR = {\n",
    "            'init_var': init_var,\n",
    "            'stiffness': stiffness,\n",
    "            'stiffness_vel': stiffness_vel,\n",
    "            'final_weight': final_weight,\n",
    "            # Parameters for guessing dynamics\n",
    "            # dU vector of accelerations, default zeros.\n",
    "            'init_acc': [],  \n",
    "            # dU vector of gains, default ones.\n",
    "            'init_gains': [],  \n",
    "        }\n",
    "        return INIT_LG_LQR\n",
    "    \n",
    "    def gen_param_POLICY_PRIOR(self, strength=1e-4):\n",
    "        # PolicyPrior\n",
    "        POLICY_PRIOR = {\n",
    "            'strength': strength,\n",
    "        }\n",
    "        return POLICY_PRIOR\n",
    "    \n",
    "    def gen_param_POLICY_PRIOR_GMM(self, min_samples_per_cluster=20.0, max_clusters=50.0, max_samples=20.0,\n",
    "                                  strength=1.0):\n",
    "        # PolicyPriorGMM\n",
    "        POLICY_PRIOR_GMM = {\n",
    "            'min_samples_per_cluster': min_samples_per_cluster,\n",
    "            'max_clusters': max_clusters,\n",
    "            'max_samples': max_samples,\n",
    "            'strength': strength,\n",
    "        }\n",
    "        return POLICY_PRIOR_GMM\n",
    "    \n",
    "    def gen_param_COST_STATE(self, RAMP_CONSTANT, l1=0.0, l2=1.0, alpha=1e-2, wp_final_multiplier=1.0, \n",
    "                            target_state=None, wp=None):\n",
    "        # CostState\n",
    "        COST_STATE = {\n",
    "            # How target cost ramps over time.\n",
    "            'ramp_option': RAMP_CONSTANT,  \n",
    "            'l1': l1,\n",
    "            'l2': l2,\n",
    "            'alpha': alpha,\n",
    "            # Weight multiplier on final time step.\n",
    "            'wp_final_multiplier': 1.0,  \n",
    "            'data_types': {\n",
    "                'JointAngle': {\n",
    "                    # Target state - must be set.\n",
    "                    'target_state': target_state,  \n",
    "                    # State weights - must be set.\n",
    "                    'wp': wp,  \n",
    "                },\n",
    "            },\n",
    "        }\n",
    "        return COST_STATE\n",
    "    \n",
    "    def gen_param_COST_SUM(self, costs=[], weights=[]):\n",
    "        # CostSum\n",
    "        COST_SUM = {\n",
    "            # A list of hyperparam dictionaries for each cost.\n",
    "            'costs': costs,  \n",
    "            # Weight multipliers for each cost.\n",
    "            'weights': weights,  \n",
    "        }\n",
    "        return COST_SUM\n",
    "    \n",
    "    def gen_param_COST_ACTION(self, wu=np.array([])):\n",
    "        # CostAction\n",
    "        COST_ACTION = {\n",
    "            # Torque penalties, must be 1 x dU numpy array.\n",
    "            'wu': np.array([]),  \n",
    "        }\n",
    "        return COST_ACTION\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required class with different functionalities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class General Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class general_utlis(object):\n",
    "    '''\n",
    "    Init function not required. Currently there for debugging.\n",
    "        init: define the class constructor of with no arguments\n",
    "    '''\n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print('-'*25)\n",
    "            print('Call to the class general_utlis has started')\n",
    "            print('-'*25)\n",
    "    \n",
    "    def check_shape(self, value, expected_shape, name=''):\n",
    "    \"\"\"\n",
    "    Throws a ValueError if value.shape != expected_shape.\n",
    "    Args:\n",
    "        value: Matrix to shape check.\n",
    "        expected_shape: A tuple or list of integers.\n",
    "        name: An optional name to add to the exception message.\n",
    "    \"\"\"\n",
    "    if value.shape != tuple(expected_shape):\n",
    "        raise ValueError('Shape mismatch %s: Expected %s, got %s' % (name, str(expected_shape), str(value.shape)))\n",
    "    \n",
    "    def extract_condition(self, hyperparams, m):\n",
    "    \"\"\"\n",
    "    Pull the relevant hyperparameters corresponding to the specified\n",
    "    condition, and return a new hyperparameter dictionary.\n",
    "    Simple explanation:\n",
    "        This function does the following:\n",
    "            If given dictionary it takes all keys and values. And makes new dicitonary out of it.\n",
    "            Additionally if one of the values is a list then it takes one element out specified by\n",
    "            the user through the parameter \"m\"\n",
    "    \"\"\"\n",
    "    return {var: val[m] if isinstance(val, list) else val for var, val in hyperparams.items()}\n",
    "\n",
    "    def approx_equal(self, a, b, threshold=1e-5):\n",
    "    \"\"\"\n",
    "    Return whether two numbers are equal within an absolute threshold.\n",
    "    Returns:\n",
    "        True if a and b are equal within threshold.\n",
    "    \"\"\"\n",
    "    return np.all(np.abs(a - b) < threshold)\n",
    "\n",
    "    def finite_differences(self, func, inputs, func_output_shape=(), epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Computes gradients via finite differences.\n",
    "    derivative = (func(x+epsilon) - func(x-epsilon)) / (2*epsilon)\n",
    "    Args:\n",
    "        func: Function to compute gradient of. Inputs and outputs can be\n",
    "            arbitrary dimension.\n",
    "        inputs: Vector value to compute gradient at.\n",
    "        func_output_shape: Shape of the output of func. Default is\n",
    "            empty-tuple, which works for scalar-valued functions.\n",
    "        epsilon: Difference to use for computing gradient.\n",
    "    Returns:\n",
    "        Gradient vector of each dimension of func with respect to each\n",
    "        dimension of input.\n",
    "    \"\"\"\n",
    "    gradient = np.zeros(inputs.shape+func_output_shape)\n",
    "    for idx, _ in np.ndenumerate(inputs):\n",
    "        test_input = np.copy(inputs)\n",
    "        test_input[idx] += epsilon\n",
    "        obj_d1 = func(test_input)\n",
    "        assert obj_d1.shape == func_output_shape\n",
    "        test_input = np.copy(inputs)\n",
    "        test_input[idx] -= epsilon\n",
    "        obj_d2 = func(test_input)\n",
    "        assert obj_d2.shape == func_output_shape\n",
    "        diff = (obj_d1 - obj_d2) / (2 * epsilon)\n",
    "        gradient[idx] += diff\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Linear Gaussian Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearGaussianPolicy(object):\n",
    "    \"\"\"\n",
    "    Time-varying linear Gaussian policy.\n",
    "    U = K*x + k + noise, where noise ~ N(0, chol_pol_covar)\n",
    "    \"\"\"\n",
    "    # TODO: Add additional noise patterns\n",
    "    def __init__(self, K, k, pol_covar, chol_pol_covar, inv_pol_covar, check_shape, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        if self.verbose: \n",
    "            print('-'*25)\n",
    "            print('Call to the class LinearGaussianPolicy has started')\n",
    "            print('-'*25)\n",
    "        \n",
    "        # Assume K has the correct shape, and make sure others match.\n",
    "        self.T = K.shape[0]\n",
    "        self.dU = K.shape[1]\n",
    "        self.dX = K.shape[2]\n",
    "\n",
    "        check_shape(k, (self.T, self.dU))\n",
    "        check_shape(pol_covar, (self.T, self.dU, self.dU))\n",
    "        check_shape(chol_pol_covar, (self.T, self.dU, self.dU))\n",
    "        check_shape(inv_pol_covar, (self.T, self.dU, self.dU))\n",
    "\n",
    "        self.K = K\n",
    "        self.k = k\n",
    "        self.pol_covar = pol_covar\n",
    "        self.chol_pol_covar = chol_pol_covar\n",
    "        self.inv_pol_covar = inv_pol_covar\n",
    "\n",
    "    def act(self, x, obs, t, noise=None):\n",
    "        \"\"\"\n",
    "        Return an action for a state.\n",
    "        Args:\n",
    "            x: State vector.\n",
    "            obs: Observation vector.\n",
    "            t: Time step.\n",
    "            noise: Action noise. This will be scaled by the variance.\n",
    "        \"\"\"\n",
    "        u = self.K[t].dot(x) + self.k[t]\n",
    "        u += self.chol_pol_covar[t].T.dot(noise)\n",
    "        return u\n",
    "\n",
    "    def fold_k(self, noise):\n",
    "        \"\"\"\n",
    "        Fold noise into k.\n",
    "        Args:\n",
    "            noise: A T x dU noise vector with mean 0 and variance 1.\n",
    "        Returns:\n",
    "            k: A T x dU bias vector.\n",
    "        \"\"\"\n",
    "        k = np.zeros_like(self.k)\n",
    "        for i in range(self.T):\n",
    "            scaled_noise = self.chol_pol_covar[i].T.dot(noise[i])\n",
    "            k[i] = scaled_noise + self.k[i]\n",
    "        return k\n",
    "\n",
    "    def nans_like(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A new linear Gaussian policy object with the same dimensions\n",
    "            but all values filled with NaNs.\n",
    "        \"\"\"\n",
    "        policy = LinearGaussianPolicy(\n",
    "            np.zeros_like(self.K), np.zeros_like(self.k),\n",
    "            np.zeros_like(self.pol_covar), np.zeros_like(self.chol_pol_covar),\n",
    "            np.zeros_like(self.inv_pol_covar)\n",
    "        )\n",
    "        policy.K.fill(np.nan)\n",
    "        policy.k.fill(np.nan)\n",
    "        policy.pol_covar.fill(np.nan)\n",
    "        policy.chol_pol_covar.fill(np.nan)\n",
    "        policy.inv_pol_covar.fill(np.nan)\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Trajectory Optimization Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Understand the part of how KL-Divergence are being computed.\n",
    "class traj_opt_utils(object):\n",
    "    '''\n",
    "    init: defines some important parameters used by this function\n",
    "    '''\n",
    "    def __init__(self, DGD_MAX_ITER=50, DGD_MAX_LS_ITER=20, DGD_MAX_GD_ITER=200, \n",
    "                 ALPHA=0.005, BETA1=0.9, BETA2=0.999, EPS=1e-8, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print('-'*25)\n",
    "            print('Call to the class traj_opt_utils has started')\n",
    "            print('-'*25)\n",
    "        # TODO: Maybe add this parameter to TrajOptLQR\n",
    "        # Constants used in TrajOptLQR.\n",
    "        self.DGD_MAX_ITER = 50\n",
    "        self.DGD_MAX_LS_ITER = 20\n",
    "        self.DGD_MAX_GD_ITER = 200\n",
    "        # Adam parameters\n",
    "        self.ALPHA, self.BETA1, self.BETA2, self.EPS = 0.005, 0.9, 0.999, 1e-8  \n",
    "        \n",
    "    def traj_distr_kl(self, new_mu, new_sigma, new_traj_distr, prev_traj_distr, tot=True):\n",
    "    \"\"\"\n",
    "    Compute KL divergence between new and previous trajectory\n",
    "    distributions.\n",
    "    Args:\n",
    "        new_mu: T x dX, mean of new trajectory distribution.\n",
    "        new_sigma: T x dX x dX, variance of new trajectory distribution.\n",
    "        new_traj_distr: A linear Gaussian policy object, new\n",
    "            distribution.\n",
    "        prev_traj_distr: A linear Gaussian policy object, previous\n",
    "            distribution.\n",
    "        tot: Whether or not to sum KL across all time steps.\n",
    "    Returns:\n",
    "        kl_div: The KL divergence between the new and previous\n",
    "            trajectories.\n",
    "    \"\"\"\n",
    "    # Constants.\n",
    "    T = new_mu.shape[0]\n",
    "    dU = new_traj_distr.dU\n",
    "\n",
    "    # Initialize vector of divergences for each time step.\n",
    "    kl_div = np.zeros(T)\n",
    "\n",
    "    # Step through trajectory.\n",
    "    for t in range(T):\n",
    "        # Fetch matrices and vectors from trajectory distributions.\n",
    "        mu_t = new_mu[t, :]\n",
    "        sigma_t = new_sigma[t, :, :]\n",
    "        K_prev = prev_traj_distr.K[t, :, :]\n",
    "        K_new = new_traj_distr.K[t, :, :]\n",
    "        k_prev = prev_traj_distr.k[t, :]\n",
    "        k_new = new_traj_distr.k[t, :]\n",
    "        chol_prev = prev_traj_distr.chol_pol_covar[t, :, :]\n",
    "        chol_new = new_traj_distr.chol_pol_covar[t, :, :]\n",
    "\n",
    "        # Compute log determinants and precision matrices.\n",
    "        logdet_prev = 2 * sum(np.log(np.diag(chol_prev)))\n",
    "        logdet_new = 2 * sum(np.log(np.diag(chol_new)))\n",
    "        prc_prev = sp.linalg.solve_triangular(\n",
    "            chol_prev, sp.linalg.solve_triangular(chol_prev.T, np.eye(dU),\n",
    "                                                  lower=True)\n",
    "        )\n",
    "        prc_new = sp.linalg.solve_triangular(\n",
    "            chol_new, sp.linalg.solve_triangular(chol_new.T, np.eye(dU),\n",
    "                                                 lower=True)\n",
    "        )\n",
    "\n",
    "        # Construct matrix, vector, and constants.\n",
    "        M_prev = np.r_[\n",
    "            np.c_[K_prev.T.dot(prc_prev).dot(K_prev), -K_prev.T.dot(prc_prev)],\n",
    "            np.c_[-prc_prev.dot(K_prev), prc_prev]\n",
    "        ]\n",
    "        M_new = np.r_[\n",
    "            np.c_[K_new.T.dot(prc_new).dot(K_new), -K_new.T.dot(prc_new)],\n",
    "            np.c_[-prc_new.dot(K_new), prc_new]\n",
    "        ]\n",
    "        v_prev = np.r_[K_prev.T.dot(prc_prev).dot(k_prev),\n",
    "                       -prc_prev.dot(k_prev)]\n",
    "        v_new = np.r_[K_new.T.dot(prc_new).dot(k_new), -prc_new.dot(k_new)]\n",
    "        c_prev = 0.5 * k_prev.T.dot(prc_prev).dot(k_prev)\n",
    "        c_new = 0.5 * k_new.T.dot(prc_new).dot(k_new)\n",
    "\n",
    "        # Compute KL divergence at timestep t.\n",
    "        kl_div[t] = max(\n",
    "            0,\n",
    "            -0.5 * mu_t.T.dot(M_new - M_prev).dot(mu_t) -\n",
    "            mu_t.T.dot(v_new - v_prev) - c_new + c_prev -\n",
    "            0.5 * np.sum(sigma_t * (M_new-M_prev)) - 0.5 * logdet_new +\n",
    "            0.5 * logdet_prev\n",
    "        )\n",
    "\n",
    "    # Add up divergences across time to get total divergence.\n",
    "    return np.sum(kl_div) if tot else kl_div\n",
    "\n",
    "\n",
    "    def traj_distr_kl_alt(self, new_mu, new_sigma, new_traj_distr, prev_traj_distr, tot=True):\n",
    "    \"\"\"\n",
    "    This function computes the same quantity as the function above.\n",
    "    However, it is easier to modify and understand this function, i.e.,\n",
    "    passing in a different mu and sigma to this function will behave properly.\n",
    "    \"\"\"\n",
    "    T, dX, dU = new_mu.shape[0], new_traj_distr.dX, new_traj_distr.dU\n",
    "    kl_div = np.zeros(T)\n",
    "\n",
    "    for t in range(T):\n",
    "        K_prev = prev_traj_distr.K[t, :, :]\n",
    "        K_new = new_traj_distr.K[t, :, :]\n",
    "\n",
    "        k_prev = prev_traj_distr.k[t, :]\n",
    "        k_new = new_traj_distr.k[t, :]\n",
    "\n",
    "        sig_prev = prev_traj_distr.pol_covar[t, :, :]\n",
    "        sig_new = new_traj_distr.pol_covar[t, :, :]\n",
    "\n",
    "        chol_prev = prev_traj_distr.chol_pol_covar[t, :, :]\n",
    "        chol_new = new_traj_distr.chol_pol_covar[t, :, :]\n",
    "\n",
    "        inv_prev = prev_traj_distr.inv_pol_covar[t, :, :]\n",
    "        inv_new = new_traj_distr.inv_pol_covar[t, :, :]\n",
    "\n",
    "        logdet_prev = 2 * sum(np.log(np.diag(chol_prev)))\n",
    "        logdet_new = 2 * sum(np.log(np.diag(chol_new)))\n",
    "\n",
    "        K_diff, k_diff = K_prev - K_new, k_prev - k_new\n",
    "        mu, sigma = new_mu[t, :dX], new_sigma[t, :dX, :dX]\n",
    "\n",
    "        kl_div[t] = max(\n",
    "                0,\n",
    "                0.5 * (logdet_prev - logdet_new - new_traj_distr.dU +\n",
    "                       np.sum(np.diag(inv_prev.dot(sig_new))) +\n",
    "                       k_diff.T.dot(inv_prev).dot(k_diff) +\n",
    "                       mu.T.dot(K_diff.T).dot(inv_prev).dot(K_diff).dot(mu) +\n",
    "                       np.sum(np.diag(K_diff.T.dot(inv_prev).dot(K_diff).dot(sigma))) +\n",
    "                       2 * k_diff.T.dot(inv_prev).dot(K_diff).dot(mu))\n",
    "        )\n",
    "\n",
    "    return np.sum(kl_div) if tot else kl_div\n",
    "\n",
    "\n",
    "    def approximated_cost(self, sample_list, traj_distr, traj_info):\n",
    "    \"\"\"\n",
    "    This function gives the LQR estimate of the cost function given the noise\n",
    "    experienced along each sample in sample_list.\n",
    "    Args:\n",
    "        sample_list: List of samples to extract noise from.\n",
    "        traj_distr: LQR controller to roll forward.\n",
    "        traj_info: Used to obtain dynamics estimate to simulate trajectories.\n",
    "    Returns:\n",
    "        mu_all: Trajectory means corresponding to each sample in sample_list.\n",
    "        predicted_cost: LQR estimates of cost of each sample in sample_list.\n",
    "    \"\"\"\n",
    "    T = traj_distr.T\n",
    "    N = len(sample_list)\n",
    "    dU = traj_distr.dU\n",
    "    dX = traj_distr.dX\n",
    "    noise = sample_list.get_noise()\n",
    "\n",
    "    # Constants.\n",
    "    idx_x = slice(dX)\n",
    "    mu_all = np.zeros((N, T, dX+dU))\n",
    "\n",
    "    # Pull out dynamics.\n",
    "    Fm = traj_info.dynamics.Fm\n",
    "    fv = traj_info.dynamics.fv\n",
    "    dyn_covar = traj_info.dynamics.dyn_covar\n",
    "\n",
    "    for i in range(N):\n",
    "        mu = np.zeros((T, dX+dU))\n",
    "        mu[0, idx_x] = traj_info.x0mu\n",
    "        for t in range(T):\n",
    "            mu[t, :] = np.hstack([\n",
    "                mu[t, idx_x],\n",
    "                (traj_distr.K[t, :, :].dot(mu[t, idx_x]) + traj_distr.k[t, :]\n",
    "                 + traj_distr.chol_pol_covar[t].T.dot(noise[i, t]))\n",
    "            ])\n",
    "            if t < T - 1:\n",
    "                mu[t+1, idx_x] = Fm[t, :, :].dot(mu[t, :]) + fv[t, :]\n",
    "        mu_all[i, :, :] = mu\n",
    "\n",
    "    # Compute cost.\n",
    "    predicted_cost = np.zeros((N, T))\n",
    "\n",
    "    for i in range(N):\n",
    "        for t in range(T):\n",
    "            predicted_cost[i, t] = traj_info.cc[t] + \\\n",
    "                    0.5 * mu_all[i,t,:].T.dot(traj_info.Cm[t, :, :]).dot(mu_all[i,t,:]) + \\\n",
    "                    mu_all[i,t,:].T.dot(traj_info.cv[t, :])\n",
    "\n",
    "    return mu_all, predicted_cost\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File: Class Bundle which is a super class and all other depending classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Add the following line because PolicyInfo class needs it:\n",
    "    from ??? import LinearGaussianPolicy\n",
    "or modify it by sending it as an argument but then either that class needs to be in same file or \n",
    "imported from other file. If you end up doing chosing latter then follow above methods that is better.\n",
    "'''\n",
    "\n",
    "class BundleType(object):\n",
    "    \"\"\"\n",
    "    This class bundles many fields, similar to a record or a mutable\n",
    "    namedtuple. \n",
    "        Which means thatif the subclass is initiated with this init function then \n",
    "        all the parameters in dictionary that are fed through variables will be made\n",
    "        into self. parameters of that subclass\n",
    "    \"\"\"\n",
    "    def __init__(self, variables):\n",
    "        for var, val in variables.items():\n",
    "            object.__setattr__(self, var, val)\n",
    "\n",
    "    # Freeze fields so new ones cannot be set.\n",
    "    def __setattr__(self, key, value):\n",
    "        if not hasattr(self, key):\n",
    "            raise AttributeError(\"%r has no attribute %s\" % (self, key))\n",
    "        object.__setattr__(self, key, value)\n",
    "        \n",
    "class IterationData(BundleType):\n",
    "    \"\"\" Collection of iteration variables. \"\"\"\n",
    "    def __init__(self):\n",
    "        variables = {\n",
    "            'sample_list': None,    # List of samples for the current iteration.\n",
    "            'traj_info': None,      # Current TrajectoryInfo object.\n",
    "            'pol_info': None,       # Current PolicyInfo object.\n",
    "            'traj_distr': None,     # Initial trajectory distribution.\n",
    "            'new_traj_distr': None, # Updated trajectory distribution.\n",
    "            'cs': None,             # Sample costs of the current iteration.\n",
    "            'step_mult': 1.0,       # KL step multiplier for the current iteration.\n",
    "            'eta': 1.0,             # Dual variable used in LQR backward pass.\n",
    "        }\n",
    "        BundleType.__init__(self, variables)\n",
    "\n",
    "\n",
    "class TrajectoryInfo(BundleType):\n",
    "    \"\"\" Collection of trajectory-related variables. \"\"\"\n",
    "    def __init__(self):\n",
    "        variables = {\n",
    "            'dynamics': None,              # Dynamics object for the current iteration.\n",
    "            'x0mu': None,                  # Mean for the initial state, used by the dynamics.\n",
    "            'x0sigma': None,               # Covariance for the initial state distribution.\n",
    "            'cc': None,                    # Cost estimate constant term.\n",
    "            'cv': None,                    # Cost estimate vector term.\n",
    "            'Cm': None,                    # Cost estimate matrix term.\n",
    "            'last_kl_step': float('inf'),  # KL step of the previous iteration.\n",
    "        }\n",
    "        BundleType.__init__(self, variables)\n",
    "        \n",
    "class PolicyInfo(BundleType):\n",
    "    \"\"\" Collection of policy-related variables. \"\"\"\n",
    "    def __init__(self, hyperparams):\n",
    "        T, dU, dX = hyperparams['T'], hyperparams['dU'], hyperparams['dX']\n",
    "        variables = {\n",
    "            'lambda_k': np.zeros((T, dU)),                      # Dual variables.\n",
    "            'lambda_K': np.zeros((T, dU, dX)),                  # Dual variables.\n",
    "            'pol_wt': hyperparams['init_pol_wt'] * np.ones(T),  # Policy weight.\n",
    "            'pol_mu': None,                       # Mean of the current policy output.\n",
    "            'pol_sig': None,                      # Covariance of the current policy output.\n",
    "            'pol_K': np.zeros((T, dU, dX)),       # Policy linearization.\n",
    "            'pol_k': np.zeros((T, dU)),           # Policy linearization.\n",
    "            'pol_S': np.zeros((T, dU, dU)),       # Policy linearization covariance.\n",
    "            'chol_pol_S': np.zeros((T, dU, dU)),  # Cholesky decomp of covar.\n",
    "            'prev_kl': None,                      # Previous KL divergence.\n",
    "            'init_kl': None,                      # The initial KL divergence, before the iteration.\n",
    "            'policy_samples': [],                 # List of current policy samples.\n",
    "            'policy_prior': None,                 # Current prior for policy linearization.\n",
    "        }\n",
    "        BundleType.__init__(self, variables)\n",
    "\n",
    "    def traj_distr(self):\n",
    "        \"\"\" Create a trajectory distribution object from policy info. \"\"\"\n",
    "        T, dU, dX = self.pol_K.shape\n",
    "        # Compute inverse policy covariances.\n",
    "        inv_pol_S = np.empty_like(self.chol_pol_S)\n",
    "        for t in range(T):\n",
    "            inv_pol_S[t, :, :] = np.linalg.solve(\n",
    "                self.chol_pol_S[t, :, :],\n",
    "                np.linalg.solve(self.chol_pol_S[t, :, :].T, np.eye(dU))\n",
    "            )\n",
    "        return LinearGaussianPolicy(self.pol_K, self.pol_k, self.pol_S,\n",
    "                self.chol_pol_S, inv_pol_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class algorithm_utils(object):\n",
    "    '''\n",
    "    Init function not required. Currently there for debugging.\n",
    "        init: define the class constructor of with no arguments\n",
    "    '''\n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print('-'*25)\n",
    "            print('Call to the class algorithm_utils has started')\n",
    "            print('-'*25)\n",
    "\n",
    "    def estimate_moments(self, X, mu, covar):\n",
    "        \"\"\" Estimate the moments for a given linearized policy. \"\"\"\n",
    "        N, T, dX = X.shape\n",
    "        dU = mu.shape[-1]\n",
    "        if len(covar.shape) == 3:\n",
    "            covar = np.tile(covar, [N, 1, 1, 1])\n",
    "        Xmu = np.concatenate([X, mu], axis=2)\n",
    "        ev = np.mean(Xmu, axis=0)\n",
    "        em = np.zeros((N, T, dX+dU, dX+dU))\n",
    "        pad1 = np.zeros((dX, dX+dU))\n",
    "        pad2 = np.zeros((dU, dX))\n",
    "        for n in range(N):\n",
    "            for t in range(T):\n",
    "                covar_pad = np.vstack([pad1, np.hstack([pad2, covar[n, t, :, :]])])\n",
    "                em[n, t, :, :] = np.outer(Xmu[n, t, :], Xmu[n, t, :]) + covar_pad\n",
    "        return ev, em\n",
    "\n",
    "\n",
    "    def gauss_fit_joint_prior(self, pts, mu0, Phi, m, n0, dwts, dX, dU, sig_reg):\n",
    "        \"\"\" Perform Gaussian fit to data with a prior. \"\"\"\n",
    "        # Build weights matrix.\n",
    "        D = np.diag(dwts)\n",
    "        # Compute empirical mean and covariance.\n",
    "        mun = np.sum((pts.T * dwts).T, axis=0)\n",
    "        diff = pts - mun\n",
    "        empsig = diff.T.dot(D).dot(diff)\n",
    "        empsig = 0.5 * (empsig + empsig.T)\n",
    "        # MAP estimate of joint distribution.\n",
    "        N = dwts.shape[0]\n",
    "        mu = mun\n",
    "        sigma = (N * empsig + Phi + (N * m) / (N + m) *\n",
    "                 np.outer(mun - mu0, mun - mu0)) / (N + n0)\n",
    "        sigma = 0.5 * (sigma + sigma.T)\n",
    "        # Add sigma regularization.\n",
    "        sigma += sig_reg\n",
    "        # Conditioning to get dynamics.\n",
    "        fd = np.linalg.solve(sigma[:dX, :dX], sigma[:dX, dX:dX+dU]).T\n",
    "        fc = mu[dX:dX+dU] - fd.dot(mu[:dX])\n",
    "        dynsig = sigma[dX:dX+dU, dX:dX+dU] - fd.dot(sigma[:dX, :dX]).dot(fd.T)\n",
    "        dynsig = 0.5 * (dynsig + dynsig.T)\n",
    "        return fd, fc, dynsig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters file\n",
    "## Need more modification not READY YET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" Hyperparameters for CartPole Problem.\"\"\"\n",
    "# from __future__ import division\n",
    "\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# import numpy as np\n",
    "\n",
    "# from gps import __file__ as gps_filepath\n",
    "# from gps.agent.box2d.agent_box2d import AgentBox2D\n",
    "# from gps.agent.box2d.arm_world import ArmWorld\n",
    "# from gps.algorithm.algorithm_badmm import AlgorithmBADMM\n",
    "# from gps.algorithm.cost.cost_state import CostState\n",
    "# from gps.algorithm.cost.cost_action import CostAction\n",
    "# from gps.algorithm.cost.cost_sum import CostSum\n",
    "# from gps.algorithm.dynamics.dynamics_lr_prior import DynamicsLRPrior\n",
    "# from gps.algorithm.dynamics.dynamics_prior_gmm import DynamicsPriorGMM\n",
    "# from gps.algorithm.policy.policy_prior_gmm import PolicyPriorGMM\n",
    "# from gps.algorithm.traj_opt.traj_opt_lqr_python import TrajOptLQRPython\n",
    "# from gps.algorithm.policy_opt.policy_opt_caffe import PolicyOptCaffe\n",
    "# from gps.algorithm.policy.lin_gauss_init import init_lqr\n",
    "# from gps.gui.config import generate_experiment_info\n",
    "# from gps.proto.gps_pb2 import JOINT_ANGLES, JOINT_VELOCITIES, END_EFFECTOR_POINTS, ACTION\n",
    "\n",
    "# SENSOR_DIMS = {\n",
    "#     JOINT_ANGLES: 1,\n",
    "#     JOINT_VELOCITIES: 1,\n",
    "#     POS: 1,\n",
    "#     VELOCITY: 1,\n",
    "#     ACTION: 1\n",
    "# }\n",
    "\n",
    "# BASE_DIR = 'checkpoints_GPS/'\n",
    "# EXP_DIR = BASE_DIR + '/Logs'\n",
    "\n",
    "\n",
    "# common = {\n",
    "#     'experiment_name': 'CartPole' + '_' + datetime.strftime(datetime.now(), '%m-%d-%y_%H-%M'),\n",
    "#     'experiment_dir': EXP_DIR,\n",
    "#     'data_files_dir': EXP_DIR + 'data_files/',\n",
    "#     'log_filename': EXP_DIR + 'log.txt',\n",
    "#     'conditions': 4,\n",
    "# }\n",
    "\n",
    "# if not os.path.exists(common['data_files_dir']):\n",
    "#     os.makedirs(common['data_files_dir'])\n",
    "\n",
    "# agent = {\n",
    "#     'type': AgentBox2D,\n",
    "#     'target_state' : np.array([0, 0]),\n",
    "#     'world' : ArmWorld,\n",
    "#     'x0': [np.array([0.5*np.pi, 0, 0, 0, 0, 0, 0]),\n",
    "#            np.array([0.75*np.pi, 0.5*np.pi, 0, 0, 0, 0, 0]),\n",
    "#            np.array([np.pi, -0.5*np.pi, 0, 0, 0, 0, 0]),\n",
    "#            np.array([1.25*np.pi, 0, 0, 0, 0, 0, 0]),\n",
    "#           ],\n",
    "#     'rk': 0,\n",
    "#     'dt': 0.05,\n",
    "#     'substeps': 1,\n",
    "#     'conditions': common['conditions'],\n",
    "#     'pos_body_idx': np.array([]),\n",
    "#     'pos_body_offset': np.array([]),\n",
    "#     'T': 100,\n",
    "#     'sensor_dims': SENSOR_DIMS,\n",
    "#     'state_include': [JOINT_ANGLES, JOINT_VELOCITIES, END_EFFECTOR_POINTS],\n",
    "#     'obs_include': [JOINT_ANGLES, JOINT_VELOCITIES, END_EFFECTOR_POINTS],\n",
    "# }\n",
    "\n",
    "# algorithm = {\n",
    "#     'type': AlgorithmBADMM,\n",
    "#     'conditions': common['conditions'],\n",
    "#     'iterations': 10,\n",
    "#     'lg_step_schedule': np.array([1e-4, 1e-3, 1e-2, 1e-2]),\n",
    "#     'policy_dual_rate': 0.2,\n",
    "#     'ent_reg_schedule': np.array([1e-3, 1e-3, 1e-2, 1e-1]),\n",
    "#     'fixed_lg_step': 3,\n",
    "#     'kl_step': 5.0,\n",
    "#     'min_step_mult': 0.01,\n",
    "#     'max_step_mult': 1.0,\n",
    "#     'sample_decrease_var': 0.05,\n",
    "#     'sample_increase_var': 0.1,\n",
    "# }\n",
    "\n",
    "# algorithm['init_traj_distr'] = {\n",
    "#     'type': init_lqr,\n",
    "#     'init_gains': np.zeros(SENSOR_DIMS[ACTION]),\n",
    "#     'init_acc': np.zeros(SENSOR_DIMS[ACTION]),\n",
    "#     'init_var': 0.1,\n",
    "#     'stiffness': 0.01,\n",
    "#     'dt': agent['dt'],\n",
    "#     'T': agent['T'],\n",
    "# }\n",
    "\n",
    "# action_cost = {\n",
    "#     'type': CostAction,\n",
    "#     'wu': np.array([1, 1])\n",
    "# }\n",
    "\n",
    "# state_cost = {\n",
    "#     'type': CostState,\n",
    "#     'data_types' : {\n",
    "#         JOINT_ANGLES: {\n",
    "#             'wp': np.array([1, 1]),\n",
    "#             'target_state': agent[\"target_state\"],\n",
    "#         },\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# algorithm['cost'] = {\n",
    "#     'type': CostSum,\n",
    "#     'costs': [action_cost, state_cost],\n",
    "#     'weights': [1e-5, 1.0],\n",
    "# }\n",
    "\n",
    "# algorithm['dynamics'] = {\n",
    "#     'type': DynamicsLRPrior,\n",
    "#     'regularization': 1e-6,\n",
    "#     'prior': {\n",
    "#         'type': DynamicsPriorGMM,\n",
    "#         'max_clusters': 20,\n",
    "#         'min_samples_per_cluster': 40,\n",
    "#         'max_samples': 20,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# algorithm['traj_opt'] = {\n",
    "#     'type': TrajOptLQRPython,\n",
    "# }\n",
    "\n",
    "# algorithm['policy_opt'] = {\n",
    "#     'type': PolicyOptCaffe,\n",
    "#     'weights_file_prefix': EXP_DIR + 'policy',\n",
    "# }\n",
    "\n",
    "# algorithm['policy_prior'] = {\n",
    "#     'type': PolicyPriorGMM,\n",
    "#     'max_clusters': 20,\n",
    "#     'min_samples_per_cluster': 40,\n",
    "#     'max_samples': 20,\n",
    "# }\n",
    "\n",
    "# config = {\n",
    "#     'iterations': 10,\n",
    "#     'num_samples': 5,\n",
    "#     'verbose_trials': 5,\n",
    "#     'verbose_policy_trials': 0,\n",
    "#     'common': common,\n",
    "#     'agent': agent,\n",
    "#     'gui_on': True,\n",
    "#     'algorithm': algorithm,\n",
    "# }\n",
    "\n",
    "# common['info'] = generate_experiment_info(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_condition(hyperparams, m):\n",
    "#     \"\"\"\n",
    "#     Pull the relevant hyperparameters corresponding to the specified\n",
    "#     condition, and return a new hyperparameter dictionary.\n",
    "#     \"\"\"\n",
    "#     return {var: val[m] if isinstance(val, list) else val for var, val in hyperparams.items()}\n",
    "\n",
    "# algorithm = {\n",
    "#     'type': 'init_lqr',\n",
    "#     'init_gains': np.zeros(6),\n",
    "#     'init_acc': np.zeros(6),\n",
    "#     'init_var': 0.1,\n",
    "#     'stiffness': 0.01,\n",
    "#     'dt': 1,\n",
    "#     'T': 2,\n",
    "# }\n",
    "# INIT_LG_LQR = {\n",
    "#     'wow': algorithm,\n",
    "#     'init_var': 1,\n",
    "#     'stiffness': 2,\n",
    "#     'stiffness_vel': 3,\n",
    "#     'final_weight': 4,\n",
    "#     # Parameters for guessing dynamics\n",
    "#     # dU vector of accelerations, default zeros.\n",
    "#     'init_acc': [],  \n",
    "#     # dU vector of gains, default ones.\n",
    "#     'init_gains': [],  \n",
    "# }\n",
    "\n",
    "\n",
    "# temp = range(10)\n",
    "# b = extract_condition(algorithm, temp)\n",
    "# # b = extract_condition(INIT_LG_LQR, temp[0])\n",
    "# print(b)\n",
    "\n",
    "\n",
    "# for var, val in INIT_LG_LQR.items():\n",
    "#     if isinstance(val, list):\n",
    "#         print('in if')\n",
    "#         a = {var: val[1]}\n",
    "#         print(a)\n",
    "#     else:\n",
    "#         print('in else')\n",
    "#         print({val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class a(object):\n",
    "#     def test_1(self):\n",
    "#         print('IN: \\n \\t Class A \\t test_1 function')\n",
    "        \n",
    "# class b(object):\n",
    "#     def __init__(self, class_a):\n",
    "#         self.obj_a = class_a()\n",
    "        \n",
    "#     def test_2(self):\n",
    "#         print('IN: \\n \\t Class B \\t test_2 function')\n",
    "#         print('HI :)'*2)\n",
    "#         self.obj_a.test_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
